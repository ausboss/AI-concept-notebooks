{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -qq install chromadb"
      ],
      "metadata": {
        "id": "2EKY06eNM6el",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34fa4771-7953-45b3-fc35-2b5b46fae385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.0/399.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The following cells creates a db and few functions.\n"
      ],
      "metadata": {
        "id": "WC8UwQxK3Zaa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5ye1fgmMuo-"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from chromadb import Settings\n",
        "from chromadb.utils import embedding_functions\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This code is setting up a Chroma database and preparing it for use."
      ],
      "metadata": {
        "id": "wvk6BjdqHIQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_directory=\"my_db_directory\"  # the directory where the database files are stored\n",
        "# set up the client to interface with the database\n",
        "client = chromadb.Client(\n",
        "    Settings(\n",
        "        persist_directory=db_directory,  # location of database files\n",
        "        chroma_db_impl=\"duckdb+parquet\",  # type of database implementation\n",
        "    )\n",
        ")\n",
        "collection_name = \"persisted_collection\"  # name of the collection of documents in the database\n",
        "ef = embedding_functions.DefaultEmbeddingFunction()  # the function to generate embeddings for queries and documents\n"
      ],
      "metadata": {
        "id": "p8kwOIOqG8S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These functions are for searching, adding to, and resetting the db"
      ],
      "metadata": {
        "id": "0VBPl8vspBAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function that takes a query text and searches it in the database\n",
        "def query_db(query_text):\n",
        "    try:\n",
        "        collection = client.get_collection(collection_name)  # load the collection of documents\n",
        "    except:\n",
        "        print(\"There was an issue loading the collection.\")\n",
        "        return\n",
        "\n",
        "    query_embedding = ef([query_text])  # generate an embedding for the query\n",
        "\n",
        "    # search the database for documents that match the query\n",
        "    results = collection.query(\n",
        "        query_embeddings=query_embedding,\n",
        "        n_results=5  # return the top 5 results\n",
        "    )\n",
        "\n",
        "    return results\n",
        "\n",
        "# function to save a list of messages to the database\n",
        "def save_messages_to_db(messages):\n",
        "    collection = client.get_or_create_collection(name=collection_name)  # load the collection, or create it if it does not exist\n",
        "\n",
        "    for message in messages:\n",
        "        embedding = ef([message])  # generate an embedding for the message\n",
        "        try:\n",
        "            search = query_db(message)\n",
        "            first_item_distance = search['distances'][0][0]\n",
        "            if first_item_distance == 0:\n",
        "                print(f\"Message '{message}' is already in database. Skipped.\")  # if the message is already in the database, do not add it again\n",
        "            else:\n",
        "                add_message_to_collection(collection, embedding, message)  # if the message is not in the database, add it\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while querying the database: {e}. Adding message '{message}' to the database.\")  # if there was an error querying the database, add the message anyway\n",
        "            add_message_to_collection(collection, embedding, message)\n",
        "\n",
        "    client.persist()  # save changes to the database\n",
        "\n",
        "# Helper function to add a message to a collection\n",
        "def add_message_to_collection(collection, embedding, message):\n",
        "    collection.add(\n",
        "        embeddings=embedding,  # the embedding of the message\n",
        "        documents=[message],  # the text of the message\n",
        "        ids=[f\"id{int(time.time())}{random.randint(0, 999999)}\"],  # a unique ID for the message\n",
        "    )\n",
        "    print(f\"Message '{message}' added to database.\")  # log that the message was added\n",
        "\n",
        "# function to delete all data in the database\n",
        "def reset_db(client):\n",
        "    client.reset()\n",
        "    print(\"db reset\")"
      ],
      "metadata": {
        "id": "RlCqkX2mEqwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save some messages to the database as embeddings"
      ],
      "metadata": {
        "id": "gEB-1-fI2EKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\"Hello, world!\", \"How are you?\", \"Goodbye!\"]\n",
        "save_messages_to_db(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cyqIoNhNWkG",
        "outputId": "82b8e4b6-a341-4644-c88c-449b4f38e3e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:01<00:00, 64.2MiB/s]\n",
            "WARNING:chromadb.db.index.hnswlib:Number of requested results 5 is greater than number of elements in index 1, updating n_results = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while querying the database: list index out of range. Adding message 'Hello, world!' to the database.\n",
            "Message 'Hello, world!' added to database.\n",
            "Message 'How are you?' added to database.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.db.index.hnswlib:Number of requested results 5 is greater than number of elements in index 2, updating n_results = 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message 'Goodbye!' added to database.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query the database\n",
        "Lower distance means closest result. The next cell has easier to read output and does the same thing"
      ],
      "metadata": {
        "id": "WvCKppph3xbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the text to query\n",
        "query_text = \"Hello\"\n",
        "\n",
        "# Use the `query_db` function from the `rememory` module to search for the query text in the database\n",
        "# This function returns a dictionary with the details of the matching documents\n",
        "results = query_db(query_text)\n",
        "\n",
        "# Print the query text and the results\n",
        "# The results include a list of document IDs, the text of the documents, and the \"distance\" from the query text for each document\n",
        "# The \"distance\" is a measure of how closely the document matches the query text, with a lower value indicating a closer match\n",
        "print(f\"Query results for '{query_text}': {results}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYT7CAyZNZl9",
        "outputId": "b696e41e-4100-4edd-ed21-3ffd083d66fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.db.index.hnswlib:Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query results for 'Hello': {'ids': [['id1689442926429066', 'id1689442926436832', 'id1689442926995715']], 'embeddings': None, 'documents': [['Hello, world!', 'Goodbye!', 'How are you?']], 'metadatas': [[None, None, None]], 'distances': [[0.6042104363441467, 1.148119568824768, 1.214174509048462]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Same as above but with a nicer output structure 😎"
      ],
      "metadata": {
        "id": "0X8tHQx44VCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the text to query\n",
        "query_text = \"Hello\"\n",
        "\n",
        "# Use the `query_db` function from the `rememory` module to search for the query text in the database\n",
        "# This function returns a dictionary with the details of the matching documents\n",
        "results = query_db(query_text)\n",
        "\n",
        "# Extract individual details (documents, ids and distances) from the results\n",
        "documents = results['documents'][0]\n",
        "ids = results['ids'][0]\n",
        "distances = results['distances'][0]\n",
        "\n",
        "# Print the query text\n",
        "print(f\"Query results for '{query_text}':\\n\")\n",
        "\n",
        "# Loop through each document and print its details\n",
        "for i in range(len(documents)):\n",
        "    print(f\"Document {i+1}:\")  # 'i+1' because 'i' starts from 0, but we want the first document to be number 1\n",
        "    print(f\"\\tID: {ids[i]}\")  # Fetch and print the ID of the current document\n",
        "    print(f\"\\tText: {documents[i]}\")  # Fetch and print the text of the current document\n",
        "    print(f\"\\tDistance: {distances[i]}\\n\")  # Fetch and print the distance of the current document\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh5VsABg4Mxz",
        "outputId": "1ac2374d-6b8f-4ba8-8a2b-4cb69ce93992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.db.index.hnswlib:Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query results for 'Hello':\n",
            "\n",
            "Document 1:\n",
            "\tID: id1689442926429066\n",
            "\tText: Hello, world!\n",
            "\tDistance: 0.6042104363441467\n",
            "\n",
            "Document 2:\n",
            "\tID: id1689442926436832\n",
            "\tText: Goodbye!\n",
            "\tDistance: 1.148119568824768\n",
            "\n",
            "Document 3:\n",
            "\tID: id1689442926995715\n",
            "\tText: How are you?\n",
            "\tDistance: 1.214174509048462\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wipe the database"
      ],
      "metadata": {
        "id": "lkLMHcwt4t_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reset_db(client)"
      ],
      "metadata": {
        "id": "hFrZGejzNbMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c808be3e-cd73-4d6e-8311-fc3c5df45b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "db reset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BONUS Summarization 🐵"
      ],
      "metadata": {
        "id": "_s8tm8sGRj1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qq install sentencepiece\n",
        "!pip -qq install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hEdfNseRpRu",
        "outputId": "61721dec-ca51-4216-a873-c7efd07f14c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change the string below to the text you want to summarize"
      ],
      "metadata": {
        "id": "5-OPUvbmw0AY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"The FitnessGram™ Pacer Test is a multistage aerobic capacity test that progressively gets more difficult as it continues. The 20 meter pacer test will begin in 30 seconds. Line up at the start. The running speed starts slowly, but gets faster each minute after you hear this signal. [beep] A single lap should be completed each time you hear this sound. [ding] Remember to run in a straight line, and run as long as possible. The second time you fail to complete a lap before the sound, your test is over. The test will begin on the word start. On your mark, get ready, start.\"\n",
        "input_text"
      ],
      "metadata": {
        "id": "zqf37VFDR689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "a928ac4e-3ff4-4e5f-8888-519dc77d3c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The FitnessGram™ Pacer Test is a multistage aerobic capacity test that progressively gets more difficult as it continues. The 20 meter pacer test will begin in 30 seconds. Line up at the start. The running speed starts slowly, but gets faster each minute after you hear this signal. [beep] A single lap should be completed each time you hear this sound. [ding] Remember to run in a straight line, and run as long as possible. The second time you fail to complete a lap before the sound, your test is over. The test will begin on the word start. On your mark, get ready, start.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import unicodedata"
      ],
      "metadata": {
        "id": "TdqHH2u_r5JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tweak the params below to fit your needs"
      ],
      "metadata": {
        "id": "Ke1pEJxLwtHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_SUMMARIZE_PARAMS = { \"temperature\": 1.0,\"repetition_penalty\": 1.0,\"max_length\": 150,\"min_length\": 50,\"length_penalty\": 1.5,\"bad_words\": [\"\\n\",'\"',\"*\",\"[\",\"]\",\"{\",\"}\",\":\",\"(\",\")\",\"<\",\">\",\"Â\",\"The text ends\",\"The story ends\",\"The text is\",\"The story is\",],}"
      ],
      "metadata": {
        "id": "0u4phxP0sRau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run this cell to set up the functions"
      ],
      "metadata": {
        "id": "SZ399zqmw7Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose GPU if available, else fall back to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pre-trained summarization transformer model from Hugging Face and load the associated tokenizer\n",
        "summarization_transformer = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "summarization_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Define a recursive function to handle texts that are too long\n",
        "def summarize_chunks(text: str, params: dict) -> str:\n",
        "    try:\n",
        "        # Try to summarize the text\n",
        "        return summarize(text, params)\n",
        "    except IndexError:\n",
        "        # If text is too long, divide it by two and try again\n",
        "        print(\"Sequence length too large for model, cutting text in half and calling again\")\n",
        "        new_params = params.copy()\n",
        "        new_params[\"max_length\"] = new_params[\"max_length\"] // 2\n",
        "        new_params[\"min_length\"] = new_params[\"min_length\"] // 2\n",
        "        # Recursive call to summarize each half of the text\n",
        "        return summarize_chunks(\n",
        "            text[: (len(text) // 2)], new_params\n",
        "        ) + summarize_chunks(text[(len(text) // 2):], new_params)\n",
        "\n",
        "# Function to generate summary\n",
        "def summarize(text: str, params: dict) -> str:\n",
        "    # Tokenize the input text\n",
        "    inputs = summarization_tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    token_count = len(inputs[0])\n",
        "\n",
        "    # Process bad words (words we don't want in the summary)\n",
        "    bad_words_ids = [\n",
        "        summarization_tokenizer(bad_word, add_special_tokens=False).input_ids\n",
        "        for bad_word in params[\"bad_words\"]\n",
        "    ]\n",
        "    # Generate summary using transformer model\n",
        "    summary_ids = summarization_transformer.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        num_beams=2,\n",
        "        max_length=max(token_count, int(params[\"max_length\"])),\n",
        "        min_length=min(token_count, int(params[\"min_length\"])),\n",
        "        repetition_penalty=float(params[\"repetition_penalty\"]),\n",
        "        temperature=float(params[\"temperature\"]),\n",
        "        length_penalty=float(params[\"length_penalty\"]),\n",
        "        bad_words_ids=bad_words_ids,\n",
        "    )\n",
        "    # Decode the summary\n",
        "    summary = summarization_tokenizer.batch_decode(\n",
        "        summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "    )[0]\n",
        "    # Normalize the summary and return\n",
        "    summary = normalize_string(summary)\n",
        "    return summary\n",
        "\n",
        "# Function to normalize string\n",
        "def normalize_string(input: str) -> str:\n",
        "    # Normalize unicode characters and remove extra spaces\n",
        "    output = \" \".join(unicodedata.normalize(\"NFKC\", input).strip().split())\n",
        "    return output\n",
        "\n",
        "# Wrapper function to handle parameter defaults and print input/outputs\n",
        "def local_summarize(text, params=None):\n",
        "    # If no parameters specified, use defaults\n",
        "    if params is None:\n",
        "        params = DEFAULT_SUMMARIZE_PARAMS.copy()\n",
        "\n",
        "    print(\"Summary input:\", text, sep=\"\\n\")\n",
        "    # Call to main summarize function\n",
        "    summary = summarize_chunks(text, params)\n",
        "    print(\"Summary output:\", summary, sep=\"\\n\")\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "F-HZBs7pRmHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finally execute the summarization"
      ],
      "metadata": {
        "id": "kzRYgN3gxKQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarize(input_text, DEFAULT_SUMMARIZE_PARAMS.copy())\n",
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "GSdcpoTYqUPA",
        "outputId": "1f92bea1-5552-4f62-c5fb-954fcad005e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The 20 meter pacer test will begin in 30 seconds. Line up at the start. The running speed starts slowly, but gets faster each minute after you hear this signal. A single lap should be completed each time you hear the signal. Remember to run in a straight line, and run as long as possible.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}